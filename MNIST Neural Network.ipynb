{"cells":[{"cell_type":"markdown","metadata":{"id":"hsywQWEI8pzj"},"source":["# Neural Network on MNIST Dataset\n","This Jupyter notebook implements a two-layer neural network and details the underlying mathematical concepts. The network is trained through the use of loss functions, gradients, and optimizers. The performance of the network is tested on the MNIST dataset. \n","\n","The implementation of the neural network uses the functionality of Numpy (http://www.numpy.org/) and Matplotlib (https://matplotlib.org/). Before running this notebook, make sure you have installed all of these packages in your local Jupyter instance.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3d6urIY6Ci2D"},"outputs":[],"source":["# Numpy and Matplotlib imports\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import math"]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"zoli2hpzp2gy"},"source":["## Multiclass Classification\n","\n","Our goal is to build a classifier to separate all 10 of the digits (classes/labels). \n","\n","We use multinomal logistic regression (aka softmax regression) where we define the posterior probability of label $y \\in \\{0,\\ldots, K-1\\}$ as \n","\n","\n","$$\\mathbf{probs(y = c | x)} = \\frac{\\exp(\\mathbf{w}_c^T\\mathbf{x})}{\\sum_{k=1}^K \\exp(\\mathbf{w}_k^T\\mathbf{x})} = \\mathbf{probs[c]}$$ \n","\n","The last layer of the network provides a probability vector $\\mathbf{probs} \\in \\mathbb{R}^K$, such that each $0 \\le \\mathbf{probs[c]} \\le 1$ and $\\sum_c \\mathbf{probs[c]} = 1$, where $\\mathbf{k}$ is the number of classes (10), and $\\mathbf{c}$ is a specific class.\n","\n","For example, if $\\mathbf{probs[5]} = 0.42$, that means there is a 42% chance that the image is a 5. For each sample, we assign our image the position of the maximum value in the probs array.\n","\n","$$\\hat{y}_i = \\arg \\max_c  \\mathbf{probs[c]} $$"]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"9vBsJizSAN5C"},"source":["### Load MNIST Dataset \n","\n","First, we must load the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) handwritten digits dataset from the Keras package. The dataset consists of 10 handwritten digits (0, 1,..., 9). It's widely used to demonstrate the simple image classification problem.\n","\n","The training and test data consists of 60000 and 10000 images, respectively, of size $28 \\times 28$ pixels. The following code segment plots 10 sample images."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load MNIST Dataset\n","def loadMNIST():\n","    from keras.datasets import mnist\n","    (xTrainMNIST, yTrainMNIST), (xTestMNIST, yTestMNIST) = mnist.load_data()\n","\n","    # Plot MNIST Examples\n","    n_img = 10\n","    plt.figure(figsize = (n_img * 2, 2))\n","    print(\"10 Random MNIST Sample Images\")\n","    plt.title(\"10 Random MNIST Sample Images\")\n","    plt.gray()\n","    for i in range(n_img):\n","        plt.subplot(1, n_img, i + 1)\n","        plt.imshow(xTrainMNIST[np.random.randint(0, xTrainMNIST.shape[0])])\n","    plt.show()\n","\n","    # Reshape data \n","    xTrainMNIST = xTrainMNIST.reshape(xTrainMNIST.shape[0], -1)\n","    xTestMNIST = xTestMNIST.reshape(xTestMNIST.shape[0], -1)\n","\n","    print(\"Training data shape:\", xTrainMNIST.shape, xTrainMNIST.shape[0], \"images, 28x28 pixels\")\n","    print(\"Test data shape:\", xTestMNIST.shape, xTestMNIST.shape[0], \"images, 28x28 pixels\")\n","\n","    return xTrainMNIST, yTrainMNIST, xTestMNIST, yTestMNIST\n","\n","xTrainMNIST, yTrainMNIST, xTestMNIST, yTestMNIST = loadMNIST()"]},{"cell_type":"markdown","metadata":{"id":"XBEj5ST72SG6"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"462ORdiFLdz2"},"source":["### Training & Test Data\n","\n","Here we pick training and test data for multi-class classification. We retrieve 1000 images from each class for a total of $N = 10000$ images and create arrays for input and output. \n","\n","We will be vectorizing the training and test images, so the size of each vector will be $784 (28\\times 28)$. After transposing the dimension of the data, the training and test dataset sizes become $784\\times N$. This will be helpful to feed it to our model based on our notations.\n","\n","```\n","# xTrain/xTest -- 784 x N array of training/test input\n","# yTrain/yTest -- 1 x N array of labels \n","```  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# EXTRACT CLASSIFIED DATASET\n","def extractAllClassificationDataset(x, y, numSamples):\n","    # Numpy arrays to store training set\n","    x_ = np.zeros((0, 784)) # Image Data\n","    y_ = np.zeros((0)) # Class Labels\n","\n","    # numSamples samples per label put in numpy arrays\n","    for label in range(10):\n","        tempX = x[y == label]\n","        tempX = tempX[:numSamples]\n","        tempY = np.full(numSamples, label)\n","        \n","        x_ = np.concatenate((x_, tempX), axis = 0)\n","        y_ = np.concatenate((y_, tempY), axis = 0)\n","\n","    return x_.T, y_\n","\n","# SELECT DATA\n","numSamples = 1000\n","xTrain, yTrain = extractAllClassificationDataset(xTrainMNIST, yTrainMNIST, numSamples)\n","xTest, yTest = extractAllClassificationDataset(xTestMNIST, yTestMNIST, numSamples)"]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"xoXHKji6L3xo"},"source":["### Network Architecture\n","\n","Our 2-layer neural network consists of an input layer with 784 nodes, a hidden layer with 256 nodes and an output layer will have 10 nodes. The first layer will use a ```sigmoid``` activation function and the second layer will use a ```softmax``` activation function.\n","\n","The equations for feedforward operation will be as follows.\n","\n","$$\\mathbf{z}^{(1)}=W^{(1)} \\mathbf{x}+ \\mathbf{b}^{(1)}\\\\\\mathbf{y}^{(1)}=\\text{sigmoid}(\\mathbf{z}^{(1)})\\\\\\mathbf{z}^{(2)}=W^{(2)}  \\mathbf{y}^{(1)}+ \\mathbf{b}^{(2)} \\\\\\mathbf{probs} = \\mathbf{y}^{(2)}=\\text{softmax}(\\mathbf{z}^{(2)})$$\n","\n","where $\\mathbf{x}\\in \\mathbb{R}^{784}$ is the input layer, $\\mathbf{y}^{(1)}\\in \\mathbb{R}^{256}$ is the hidden layer, $\\mathbf{y}^{(2)} \\in \\mathbb{R}$ is the output layer, $W^{(1)}\\in \\mathbb{R}^{256\\times 784}$ is the first layer weights, $W^{(2)}\\in \\mathbb{R}^{10\\times 256}$ is the second layer weights, $\\mathbf{b}^{(1)}\\in \\mathbb{R}^{256}$ is the first layer bias, $\\mathbf{b}^{(2)}\\in \\mathbb{R}^{10}$ is the second layer bias vector."]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"2Fqsp1BvL3xp"},"source":["### Network initialization\n","\n","We initialize the weights for $W^{(1)}$ and $W^{(2)}$ with random values drawn from normal distribution with zero mean and 0.01 standard deviation. We will initialize bias vectors $\\mathbf{b}^{(1)}$ and $\\mathbf{b^{(2)}}$ with zero values. \n","\n","We can fix the seed for random initialization for reproducibility."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# INITIALIZE NEURAL NETWORK\n","def TwoLayerNetwork(layer_dims=[784, 256, 10]):\n","    # Initialize Normally Distributed Weights\n","    W1 = np.random.normal(0, 0.01, (layer_dims[1], layer_dims[0])) # (256, 784)\n","    W2 = np.random.normal(0, 0.01, (layer_dims[2], layer_dims[1])) # (10, 256)\n","\n","    # Initialize Biases to 0\n","    b1 = np.zeros(layer_dims[1]) # (256, )\n","    b2 = np.zeros(layer_dims[2]) # (10, ))\n","\n","    # Return array of weights/biases\n","    params = np.array([W1, b1, W2, b2], dtype=object)\n","    return params"]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"0xLDm_wkAN5F"},"source":["### Sigmoid Activation Function \n","The ```sigmoid``` activation function is written as \n","\n","$$ \\varphi(z) = \\frac{1}{1+e^{-z}}$$\n","\n","Note that derivative of ```sigmoid``` is $\\varphi'(z) = \\varphi(z) (1-\\varphi(z))$. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# SIGMOID ACTIVATION FUNCTION\n","def sigmoid(Z):\n","    # Input: Z numpy.ndarray\n","    Y = 1/(1 + np.exp(-Z))\n","    return Y"]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"vxGPKVAUBO-w"},"source":["### Softmax Activation Function\n","The softmax activation function is a multinomal extension of the sigmoid function that maps a vector of length $K$ to a probability vector.\n","\n","We can define ```softmax``` function on a vector $\\mathbf{z} \\in \\mathbb{R}^K$ as $\\mathbf{p} = \\text{softmax}(\\mathbf{z})$: \n","\n","$$\\mathbf{p}_c(\\mathbf{z}) = \\frac{\\exp(\\mathbf{z}_c)}{\\sum_{k=1}^K \\exp(\\mathbf{z}_k)}$$\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wezeelLcBSCo"},"outputs":[],"source":["# SOFTMAX ACTIVATION FUNCTION\n","def softmax(probs): \n","    # probs -- K x N numpy.ndarray, K is the number of classes, N is the number of samples\n","    probs = np.exp(probs) / np.sum(np.exp(probs), axis = 0)\n","    return probs"]},{"cell_type":"markdown","metadata":{},"source":["The derivative of the ```softmax``` function with respect to any input can be written as \n","\n","$$ \\frac{\\partial \\mathbf{p}_i}{\\partial \\mathbf{z}_j} = \\begin{cases} \\mathbf{p}_i(1-\\mathbf{p}_j) & i = j \\\\ \\mathbf{p}_i (-\\mathbf{p}_j) & i \\ne j. \\end{cases}$$"]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"9J53N9_wWMUf"},"source":["### Stable Softmax\n","The numerical range of floating point numbers in numpy is limited. For `float64` the upper bound is $10^{308}$. For exponential, its not difficult to overshoot that limit, in which case python returns `nan`.\n","\n","To make our softmax function numerically stable, we normalize the values in the vector by multiplying the numerator and denominator with a constant `C` as\n","\n","\\begin{align*}\n","\\mathbf{p}_c  &= \\frac{\\exp(\\mathbf{z}_c)}{\\sum_{k=1}^K \\exp(\\mathbf{z}_k)} \\\\\n","& = \\frac{C\\exp(\\mathbf{z}_c)}{C\\sum_{k=1}^K \\exp(\\mathbf{z}_k)}\\\\\n","& = \\frac{\\exp(\\mathbf{z}_c + \\log C)}{C\\sum_{k=1}^K \\exp(\\mathbf{z}_k + \\log C)}.\n","\\end{align*}\n","\n","We can choose an arbitrary value for `log(C)` term, but generally `log(C) = −max(z)` is chosen\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JzmyZdoqXO_v"},"outputs":[],"source":["# STABLE SOFTMAX ACTIVATION FUNCTION\n","def stable_softmax(probs): \n","    # probs -- K x N numpy.ndarray, K is the number of classes, N is the number of samples\n","    probs = np.exp(probs - np.max(probs)) / np.sum(np.exp(probs - np.max(probs)), axis = 0)\n","    return probs"]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"C0qcy_JAArEo"},"source":["### Multiclass Cross Entropy Loss Function\n","\n","We want to minimize the ```cross entropy loss``` using the true labels and predicted labels of a batch of N samples. The multi-class ```cross entropy loss``` for $i^{th}$ sample can be written as\n","\n","$$Loss_i = -\\sum_c \\mathbf{1}(y_i = c) \\log \\mathbf{p}_c $$\n","where $y_i$ is the true label and \n","\n","$$\\mathbf{1}(y_i = c) = \\begin{cases} 1 & y_i =c \\\\ 0 & \\text{otherwise} \\end{cases}$$ \n","is an indicator function. \n","\n","We can find the average loss for a batch of N samples as $Loss=\\frac{1}{N}\\sum_{i=1}^{N} Loss_i$. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tebVzE2SAoTB"},"outputs":[],"source":["# CROSS ENTROPY LOSS FUNCTION FOR MULTIPLE CLASSES\n","def MultiClassCrossEntropyLoss(Y_true, probs):\n","  # probs -- K x N array\n","  # Y_true -- 1 x N array \n","  # loss -- sum Loss_i over N samples \n","\n","  N = Y_true.shape[0] # N Samples\n","  p = stable_softmax(probs)\n","  log_likelihood = -np.log(p[Y_true.astype(int), range(N)])\n","  loss = np.sum(log_likelihood)/N\n","  return loss"]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"tbRPrVLpjFIo"},"source":["### Derivative of the Cross Entropy Loss \n","\n","Let's assume that $\\mathbf{probs} = \\text{softmax}(\\mathbf{z})$. \n","\n","The derivative of the loss w.r.t. $\\mathbf{probs}_j$ can be written as \n","$$\\frac{\\partial Loss_i }{\\partial \\mathbf{probs}_j} = \\begin{cases} -1/\\mathbf{probs}_j & j = y_i \\\\ 0 & j \\ne y_i \\end{cases}. $$\n","\n","The _total derivative_ is used to compute the derivative of the loss for $i^{th}$ sample w.r.t. $j^{th}$ entry in $\\mathbf{z}$ as\n","\n","\\begin{align*}\n","\\frac{\\partial Loss_i}{\\partial \\mathbf{z}_j} = \\sum_c \\frac{\\partial Loss_i}{\\partial \\mathbf{probs}_c}\\frac{\\partial \\mathbf{probs}_c}{\\partial \\mathbf{z}_j}.\n","\\end{align*}\n","\n","From our discussion above, we know that the $\\frac{\\partial Loss_i}{\\partial \\mathbf{probs}_c} = 0$ if $c \\ne y_i$. \n","\n","\n","\\begin{align*}\n","\\frac{\\partial Loss_i}{\\partial \\mathbf{z}_j} &= -\\frac{1}{\\mathbf{probs}_c} \\frac{\\partial \\mathbf{probs}_c}{\\partial \\mathbf{z}_j} \\\\\n","& = \\begin{cases} \\mathbf{probs}_j - 1 & j = y_i \\\\ \\mathbf{probs}_j & j \\ne y_i. \\end{cases}\n","\\end{align*}\n","\n","Therefore, $$\\delta^{(2)} = \\nabla_{\\mathbf{z}^{(2)}} Loss_i = \\mathbf{probs} - \\mathbf{1}_{y_i}.$$\n","\n","where $\\mathbf{1}_{y_i}$ is a __one-hot vector__ that has length $K$ and is zero everywhere except 1 at index same as $y_i$. \n"]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"av7l9tOKUuAN"},"source":["### Forward Propagation \n","This is the forward pass for our two layer network. Each layer consists of an affine function (fully-connected layer) followed by an activation function (```sigmoid``` or ```softmax```). \n","\n","The forward propagation function returns the intermediate results ($\\mathbf{x}, \\mathbf{z}^{(1)}, \\mathbf{y}^{(1)}, \\mathbf{z}^{(2)}$) in addition to the final output ($\\mathbf{probs = y}^{(2)}$). The intermediate outputs are used for the backpropagation step."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kLSoTwFkUuAO"},"outputs":[],"source":["# FORWARD PROPAGATION\n","def forward(X, params):\n","    # Inputs:\n","      # X -- 784 x N array \n","      # params\n","        # W1 -- 256 x 784 matrix\n","        # b1 -- 256 x 1 vector\n","        # W2 -- 10 x 256 matrix\n","        # b2 -- 10 x 1 vector \n","      \n","    # Outputs:\n","      # probs -- 10 x N output\n","\n","    Z1 = np.matmul(params[0], X) + params[1][:, np.newaxis] # W1X + b1\n","    Y1 = sigmoid(Z1) # Output of layer 1\n","    Z2 = np.matmul(params[2], Y1) + params[3][:, np.newaxis] # W2Y1 + b2\n","    probs = stable_softmax(Z2) # Final output\n","\n","    # Put all intermediate values in array and return\n","    intermediate = np.array([X, Z1, Y1, Z2], dtype=object)\n","    return probs, intermediate"]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"Z3QjvXHIUuAO"},"source":["### Backpropagration\n","\n","The backpropagation step for the two layer neural network is implemented using the ```softmax``` layer and ```cross entropy loss``` function. \n","\n","\n","The gradient of the Loss w.r.t. $W^{(l)},\\mathbf{b}^{(l)}$ for $l = 1,2$ for a single training sample can be written as  \n","\n","$$\\nabla_{W^{(l)}} Loss_i = \\delta^{(l)} \\mathbf{y}^{(l-1)T},$$  \n","$$\\nabla_{\\mathbf{b}^{(l)}} Loss_i = \\delta^{(l)},$$\n","\n","\n","where \n","$$\\delta^{(l)} = \\nabla_{\\mathbf{z}^{(l)}} Loss = \\nabla_{\\mathbf{y}^{(l)}} Loss \\odot \\varphi'(\\mathbf{z}^{(l)}).$$ \n","\n","For an $i^{th}$ sample, $\\delta^{(2)} = \\nabla_{\\mathbf{z}^{(2)}} Loss_i = \\mathbf{probs} - \\mathbf{1}_{y_i},$ where $\\mathbf{1}_{y_i}$ is a __one-hot vector__ that has length $K$ and is zero everywhere except 1 at index same as $y_i$, and $\\mathbf{probs}$ is the output probability vector for the $i^{th}$ sample. \n","\n","\n","Once we have the gradients $\\nabla_{W^{(l)}} Loss_i, \\nabla_{\\mathbf{b}^{(l)}} Loss_i$ for all $i$. We can compute their average to compute the gradient of the total loss function as\n","\n","$$\\nabla_{W^{(l)}} Loss = \\frac{1}{N} \\sum_i \\nabla_{W^{(l)}} Loss_i, $$\n","$$ \\nabla_{\\mathbf{b}^{(l)}} Loss = \\frac{1}{N} \\sum_i  \\nabla_{\\mathbf{b}^{(l)}} Loss_i.$$"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# BACKPROPAGATION\n","def backward(Y_true, probs, intermediate, params):\n","    # Inputs: \n","      # Y_true -- 1 x N true labels\n","      # probs -- 10 x N output of the last layer\n","      # intermediate -- X, Z1, Y1, Z2 \n","      # params -- W1, b1, W2, b2 \n","    \n","    # Outputs: \n","      # grads -- [grad_W1, grad_b1, grad_W2, grad_b2]\n","\n","    # Num Samples\n","    N = np.size(Y_true)\n","\n","    # LAYER 2 (l = L)\n","    delta2 = np.zeros((10, N))\n","    Y1 = intermediate[2]\n","\n","    # Compute delta2\n","    for i in range(N):\n","      oneHot = np.zeros(10)\n","      oneHot[Y_true[i].astype(int)] = 1\n","      delta2[:, i] = probs[:, i] - oneHot\n","\n","    # Compute b2GradientAverage/W2GradientAverage\n","    b2GradientAverage = np.zeros((10))\n","    for j in range(10):\n","      b2GradientAverage[j] = np.average(delta2[j, :])\n","    W2GradientAverage = (1/N) * np.matmul(delta2, Y1.T) # (10, 256)\n","\n","    # LAYER 1 (l < L)\n","    W2 = params[2]\n","    Z1 = intermediate[1]\n","    \n","    # Compute delta1\n","    delta1 = np.matmul(W2.T, delta2) * (sigmoid(Z1) * (1 - sigmoid(Z1))) # (256, 10000)\n","\n","    # Compute b1GradientAverage\n","    b1GradientAverage = np.zeros((256))\n","    for j in range(256):\n","      b1GradientAverage[j] = np.average(delta1[j, :])\n","\n","    # Compute W1GradientAverage\n","    X = intermediate[0]\n","    W1GradientAverage = (1/N) * np.matmul(delta1, X.T) # (256, 784)\n","\n","    # Put weight/bias gradients in array and return\n","    grads = np.array([W1GradientAverage, b1GradientAverage, W2GradientAverage, b2GradientAverage], dtype=object)\n","    return grads"]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"GlHu8oIaAN5G"},"source":["### Gradient Descent Optimizer\n","We will use a standard gradient descent-based optimizer to minimize the ```cross entropy loss``` function. The learning rate may be adjusted to provide the best training/validation performance. The same learning rate is applied for all weights.\n","\n","$W^1, \\mathbf{b}^1, W^2, \\mathbf{b}^2$ are updated as \n","$$ W^1 \\gets W^1 - \\alpha \\nabla_{W^1} Loss $$\n","$$ \\mathbf{b}^1 \\gets \\mathbf{b}^1 - \\alpha \\nabla_{\\mathbf{b}^1} Loss $$ \n","$$ W^2 \\gets W^2 - \\alpha \\nabla_{W^2} Loss $$ \n","$$ \\mathbf{b}^2 \\gets \\mathbf{b}^2 - \\alpha \\nabla_{\\mathbf{b}^2} Loss $$ \n","where $\\alpha$ is the learning rate. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# GRADIENT DESCENT OPTIMIZER\n","def GD(params, grads, learning_rate):\n","    # New params = old params - (learning rate (α) * gradient of Loss computed at old params)\n","    # params -- W1, b1, W2, b2 \n","    params[0] = params[0] - (learning_rate * grads[0]) #W1 - αLossW1 (256, 784)\n","    params[1] = params[1] - (learning_rate * grads[1]) #b1 - αLossb1 (256, )\n","    params[2] = params[2] - (learning_rate * grads[2]) #W2 - αLossW2 (1, 256)\n","    params[3] = params[3] - (learning_rate * grads[3]) #b2 - αLossb2 (1, )\n","    return params"]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"1DTxxcDTvVQD"},"source":["### Training the Model\n","To train our multi-class classificaiton model, we use the forward and backward propagation functions with our gradient descent optimizer. \n","\n","First, we specify the number of nodes in the layers, number of epochs, and learning rate."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xmhkhUisUuAQ"},"outputs":[],"source":["# Specify layer dimensions, number of epochs, and learning rate\n","layerDims = [xTrain.shape[0], 256, 10]\n","epochs = 250\n","lr = 0.025"]},{"cell_type":"markdown","metadata":{},"source":["The network is intialized given our layerDims."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize Neural Network\n","params = TwoLayerNetwork(layerDims)"]},{"cell_type":"markdown","metadata":{"id":"9DZAfG5QUuAQ"},"source":["Then, we train the network for the number of epochs specified above. In every epoch, we execute the following:\n","1. Calculate a forward pass to get estimated labels\n","2. Use the estimated labels tp calculate and record loss for every epoch\n","3. Use backpropagation to calculate gradients\n","4. Use gradient descent to update the weights and biases\n","\n","The loss value after every epoch is stored in the ```lossHistory``` array and is printed after every 25 epochs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P1zSP6g8UuAQ"},"outputs":[],"source":["# Truncate float to n decimal places\n","def truncate(f, n):\n","    s = '{}'.format(f)\n","    if 'e' in s or 'E' in s:\n","        return '{0:.{1}f}'.format(f, n)\n","    i, p, d = s.partition('.')\n","    return '.'.join([i, (d+'0'*n)[:n]])\n","\n","# True labels (N x 1)\n","Y_true = yTrain\n","\n","# Calculate loss for each epoch\n","print(\"TRAINING MODEL....\")\n","lossHistory = np.zeros(epochs)\n","for i in range(epochs):\n","  # Forward Pass\n","  probs, intermediate = forward(xTrain, params) # Y2 1 x N output, intermediate[X, Z1, Y1, Z2], params[W1, b1, W2, b2]\n","\n","  # Backpropagation\n","  grads = backward(Y_true, probs, intermediate, params)\n","\n","  # Gradient Descent Update\n","  params = GD(params, grads, lr)\n","\n","  # Calculate Loss\n","  lossHistory[i] = MultiClassCrossEntropyLoss(Y_true, probs)\n","  if i % 25 == 0: # Print loss every 25 epochs\n","    print(\"Loss at\", i , \"epoch:\", truncate(lossHistory[i], 4))\n"]},{"cell_type":"markdown","metadata":{"id":"mAtKFEjTUuAQ"},"source":["We plot the recorded loss values vs epochs. We can observe that the training loss decreases with the epochs. Optimally, we'd like the loss value to be as close to 0 as possible."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hw14IunrUuAQ"},"outputs":[],"source":["# Plot Loss vs Epoches\n","plt.figure()\n","plt.plot(lossHistory)\n","print(\"Epochs vs Training Loss\")\n","plt.title(\"Epochs vs Training Loss\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Training Loss\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"Pf4fN1udUuAR"},"source":["### Accuracy Evaluation\n","Now that the network is finished training, we evaluate the accuracy from the trained model. We feed training data and test data to the forward model along with our new trained parameters. \n","\n","To convert the probability output of the forward pass into labels, we can assign the label based on the maximum probability. \n","\n","$$\\hat{y}_i = \\arg \\max_c  \\mathbf{probs[c]} $$\n","\n","It is always possible to increase accuracy by reducing our Loss further with more epochs or an adjusted learning rate."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Compute the training/test accuracy and return correct/wrong indexes\n","def computeAccuracy(Y_true, probs):\n","    N = probs.shape[1]\n","    correct = 0\n","    correctIndexes = np.zeros(0)\n","    wrongIndexes = np.zeros(0)\n","    classifications = np.zeros(0)\n","\n","    for i in range(N):\n","        classifications = np.append(classifications, np.argmax(probs[:, i]))\n","        if(classifications[i] == Y_true[i]):\n","            correct += 1\n","            correctIndexes = np.append(correctIndexes, i)\n","        else:\n","            wrongIndexes = np.append(wrongIndexes, i)\n","    \n","    accuracy = (correct/N) * 100\n","    return accuracy, classifications, correctIndexes.astype(np.int64), wrongIndexes.astype(np.int64)\n","\n","# TRAINING ACCURACY\n","trainingAccuracy, trainingClassifications, correctTrainingIndexes, wrongTrainingIndexes = computeAccuracy(Y_true, probs)\n","print(\"TRAINING ACCURACY:\", truncate(trainingAccuracy, 4), \"%\")\n","\n","# TEST ACCURACY\n","# Use optimized parameters on a forward pass of the test data set\n","probs_test = forward(xTest, params)[0]\n","testAccuracy, testClassifications, correctTestIndexes, wrongTestIndexes = computeAccuracy(yTest, probs_test)\n","print(\"TEST ACCURACY:\", truncate(testAccuracy, 4), \"%\")"]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"iLjwg5wjMcyB"},"source":["### Visualization of Correct/Miscalassified Images\n","\n","Next, we look at some images from training and test sets that were correctly or incorrectly classified. We can observe that incorrectly classified images could look very distinct compared to their correctly classified counterparts."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-NtpDz-Mkub"},"outputs":[],"source":["# Plot correctly & wrongly classified images from the dataset\n","def plotClassifiedImages(n_img, correctIndexes, wrongIndexes, dataset, typeString):\n","    newDataset = dataset.reshape(28, 28, dataset.shape[1])\n","    totalRows = int(math.ceil(n_img / 10))\n","\n","    # Correctly Classified\n","    print(n_img, \"correctly classified images from\", typeString, \"dataset\")\n","    maxImages = bool(False)\n","    displayedImages = np.zeros(0)\n","    for row in range(totalRows): # For each row\n","        imagesOnRow = min(10, n_img - (row * 10)) # 10 max images per row\n","        plt.figure(figsize = (imagesOnRow * 2, 2))\n","        plt.gray()\n","\n","        # Plot imagesOnRow images\n","        for i in range(imagesOnRow):\n","            plt.subplot(1, imagesOnRow, i + 1)\n","            imageIndex = correctIndexes[np.random.randint(0, correctIndexes.size)]\n","\n","            # Don't plot images that have already been displayed\n","            while(imageIndex in displayedImages):\n","                imageIndex = correctIndexes[np.random.randint(0, correctIndexes.size)]\n","            displayedImages = np.append(displayedImages, imageIndex)\n","\n","            # Random correctly classified image\n","            image = newDataset[:, :, imageIndex]\n","            plt.imshow(image)\n","\n","            # No more images left\n","            if (displayedImages.size == correctIndexes.size):\n","                maxImages = bool(True)\n","                break\n","\n","        plt.show()\n","        if(maxImages):\n","            print(\"Maximum images displayed:\", wrongIndexes.size)\n","            break\n","        row += 1\n","\n","    # Wrongly Classified\n","    print(n_img, \"wrongly classified images from\", typeString, \"dataset\")\n","    maxImages = bool(False)\n","    displayedImages = np.zeros(0)\n","    for row in range(totalRows): # For each row\n","        imagesOnRow = min(10, n_img - (row * 10)) # 10 max images per row\n","        plt.figure(figsize = (imagesOnRow * 2, 2))\n","        plt.gray()\n","\n","        # Plot imagesOnRow images\n","        for i in range(imagesOnRow):\n","            plt.subplot(1, imagesOnRow, i + 1)\n","            imageIndex = wrongIndexes[np.random.randint(0, wrongIndexes.size)]\n","\n","            # Don't plot images that have already been displayed\n","            while(imageIndex in displayedImages):\n","                imageIndex = wrongIndexes[np.random.randint(0, wrongIndexes.size)]\n","            displayedImages = np.append(displayedImages, imageIndex)\n","\n","            # Random wrongly classified image\n","            image = newDataset[:, :, imageIndex]\n","            plt.imshow(image)\n","\n","            # No more images left\n","            if (displayedImages.size == wrongIndexes.size):\n","                maxImages = bool(True)\n","                break\n","            \n","        plt.show()\n","        if(maxImages):\n","            print(\"Maximum images displayed:\", wrongIndexes.size)\n","            break\n","        row += 1\n","\n","# Plot five correctly & wrongly classified images from the training and test sets\n","plotClassifiedImages(5, correctTrainingIndexes, wrongTrainingIndexes, xTrain, \"training\")\n","plotClassifiedImages(5, correctTestIndexes, wrongTestIndexes, xTest, \"test\")"]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"markdown","metadata":{},"source":["### Visualize More Images\n","Finally, the program prompts the user if they'd like to view more correctly/incorrectly classified images. The user can choose which dataset they'd like to view images from, how many images they'd like to display, and whether they'd like to view images from a specific class (digit)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# View more correctly & wrongly classified images from either dataset\n","while(True):\n","    choice = input(\"Would you like to view more images? (y/n) \")\n","    if choice.lower() == \"y\":\n","        \n","        # Select training or test dataset\n","        trainingOrTest = input(\"Which dataset? (training/test) \")\n","        while(trainingOrTest.lower() != \"training\" and trainingOrTest.lower() != \"test\"):\n","            trainingOrTest = input(\"Select a valid dataset (training/test) \")\n","\n","        # Select number of images to plot\n","        numImages = int(input(\"How many images would you like to display? \"))\n","        while(numImages < 0 or numImages > 200):\n","            numImages = int(input(\"Select a valid number of images (Max = 200) \"))\n","\n","        # Select specific number to plot\n","        numberChoice = input(\"Would you like to plot a specific number? (y/n) \")\n","        while(numberChoice.lower() != \"y\" and numberChoice.lower() != \"n\"):\n","            numberChoice = input(\"Select a valid choice (y/n) \")\n","\n","        # Initialization for arrays to use in plotClassifiedImages()\n","        dataset = np.zeros((784, 0))\n","        correctIndexes = np.zeros(0)\n","        wrongIndexes = np.zeros(0)\n","\n","        # Build new dataset and indexes for 1 number\n","        if numberChoice.lower() == \"y\":\n","            number = int(input(\"Which number would you like to plot? (0-9) \"))\n","            while(number < 0 or number > 9):\n","                number = int(input(\"Select a valid number (0-9) \"))\n","\n","            # Build from training dataset\n","            if trainingOrTest.lower() == \"training\":\n","                currentIndex = 0\n","                for i in range(correctTrainingIndexes.size):\n","                    if(yTrain[correctTrainingIndexes[i]] == number):\n","                        correctIndexes = np.append(correctIndexes, currentIndex)\n","                        dataset = np.append(dataset, np.vstack(xTrain[:, correctTrainingIndexes[i]]), 1)\n","                        currentIndex += 1\n","                for i in range(wrongTrainingIndexes.size):\n","                    if(yTrain[wrongTrainingIndexes[i]] == number):\n","                        wrongIndexes = np.append(wrongIndexes, currentIndex)\n","                        dataset = np.append(dataset, np.vstack(xTrain[:, wrongTrainingIndexes[i]]), 1)\n","                        currentIndex += 1\n","                \n","            # Build from test dataset\n","            elif trainingOrTest.lower() == \"test\":\n","                currentIndex = 0\n","                for i in range(correctTestIndexes.size):\n","                    if(yTest[correctTestIndexes[i]] == number):\n","                        correctIndexes = np.append(correctIndexes, currentIndex)\n","                        dataset = np.append(dataset, np.vstack(xTest[:, correctTestIndexes[i]]), 1)\n","                        currentIndex += 1\n","                for i in range(wrongTestIndexes.size):\n","                    if(yTest[wrongTestIndexes[i]] == number):\n","                        wrongIndexes = np.append(wrongIndexes, currentIndex)\n","                        dataset = np.append(dataset, np.vstack(xTest[:, wrongTestIndexes[i]]), 1)\n","                        currentIndex += 1\n","            \n","        # Use entire training/test dataset\n","        else: \n","            if trainingOrTest.lower() == \"training\":\n","                dataset = xTrain\n","                correctIndexes, wrongIndexes = correctTrainingIndexes, wrongTrainingIndexes\n","            elif trainingOrTest.lower() == \"test\":\n","                dataset = xTest\n","                correctIndexes, wrongIndexes = correctTestIndexes, wrongTestIndexes\n","\n","        # PLOT\n","        plotClassifiedImages(numImages, correctIndexes.astype(np.int64), wrongIndexes.astype(np.int64), dataset, trainingOrTest.lower())\n","    else:\n","        print(\"Exiting program....\")\n","        break\n"]}],"metadata":{"colab":{"collapsed_sections":["UKc8K9_57UEz","JL5tIX4c9z6s","coxduSVB9gut","9vBsJizSAN5C","HdeO0YieAN5E","tSGHPdySbpbj","tbRPrVLpjFIo","xoXHKji6L3xo","1DTxxcDTvVQD","Pf4fN1udUuAR","iLjwg5wjMcyB","tU1ZfPTjv-jm"],"name":"CS171 hw3.ipynb","provenance":[{"file_id":"1zMWHS8t_QoWT_0XkpJnNevcye4ucUTTD","timestamp":1636961778124},{"file_id":"1_Vdcf9Cv6RssMIoBCqZWFEKbHofiCQgz","timestamp":1635615505459},{"file_id":"1Z6lYMpb4bygcDckti4rImGrfWPEzu4GJ","timestamp":1620515473539}]},"kernelspec":{"display_name":"Python 3.10.5 ('venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"vscode":{"interpreter":{"hash":"231f9ff7750c74e37037f2093902d792f3189ec9b93b1eacbfce0492138b664f"}}},"nbformat":4,"nbformat_minor":0}
